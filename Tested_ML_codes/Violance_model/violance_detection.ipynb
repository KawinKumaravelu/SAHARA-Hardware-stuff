{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afb8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Cell 1: Imports\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, TimeDistributed, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf45979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Cell 2: Dataset paths\n",
    "violence_path = \"Violence\"\n",
    "nonviolence_path = \"NonViolence\"\n",
    "\n",
    "IMG_SIZE = 112     # resize frames\n",
    "SEQUENCE_LEN = 20  # no. of frames per video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc2ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Cell 3: Function to load videos\n",
    "def load_videos_from_folder(folder, label):\n",
    "    X, y = [], []\n",
    "    for file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, file)\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        frames = []\n",
    "        while len(frames) < SEQUENCE_LEN and cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "            frame = frame / 255.0\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        if len(frames) == SEQUENCE_LEN:\n",
    "            X.append(frames)\n",
    "            y.append(label)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e1d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Cell 4: Collect video file paths instead of loading everything into memory\n",
    "\n",
    "# Get all Violence and NonViolence video file paths\n",
    "violence_files = [os.path.join(violence_path, f) for f in os.listdir(violence_path) if f.endswith(\".mp4\")]\n",
    "nonviolence_files = [os.path.join(nonviolence_path, f) for f in os.listdir(nonviolence_path) if f.endswith(\".mp4\")]\n",
    "\n",
    "# Labels: 1 = Violence, 0 = NonViolence\n",
    "video_paths = violence_files + nonviolence_files\n",
    "labels = [1] * len(violence_files) + [0] * len(nonviolence_files)\n",
    "\n",
    "print(\"Total videos found:\", len(video_paths))\n",
    "print(\"Violence videos:\", len(violence_files))\n",
    "print(\"NonViolence videos:\", len(nonviolence_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Cell 5: Train-test split (only paths & labels)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_paths, test_paths, y_train, y_test = train_test_split(\n",
    "    video_paths, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training videos:\", len(train_paths))\n",
    "print(\"Testing videos:\", len(test_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676efa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Cell 6: Data Generator + Model definition\n",
    "\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "\n",
    "# ====== Data Generator ======\n",
    "class VideoDataGenerator(Sequence):\n",
    "    def __init__(self, video_paths, labels, batch_size=8, sequence_len=20, img_size=112, n_classes=2):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_len = sequence_len\n",
    "        self.img_size = img_size\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.video_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_paths = self.video_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        X, y = [], []\n",
    "        for path, label in zip(batch_paths, batch_labels):\n",
    "            cap = cv2.VideoCapture(path)\n",
    "            frames = []\n",
    "            while len(frames) < self.sequence_len and cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = cv2.resize(frame, (self.img_size, self.img_size)).astype(\"float32\") / 255.0\n",
    "                frames.append(frame)\n",
    "            cap.release()\n",
    "\n",
    "            if len(frames) == self.sequence_len:   # only keep valid sequences\n",
    "                X.append(frames)\n",
    "                y.append(label)\n",
    "\n",
    "        return np.array(X, dtype=\"float32\"), to_categorical(y, self.n_classes)\n",
    "\n",
    "# ====== Model Definition ======\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, GlobalAveragePooling2D, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(112, 112, 3))\n",
    "\n",
    "model = Sequential([\n",
    "    TimeDistributed(base_model, input_shape=(20, 112, 112, 3)),\n",
    "    TimeDistributed(GlobalAveragePooling2D()),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(2, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(1e-4), metrics=[\"accuracy\"])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcacb735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“Œ Cell 7: Training with generators\n",
    "\n",
    "# Create generators\n",
    "train_gen = VideoDataGenerator(train_paths, y_train, batch_size=8, sequence_len=20, img_size=112)\n",
    "test_gen  = VideoDataGenerator(test_paths, y_test, batch_size=8, sequence_len=20, img_size=112)\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=test_gen,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save(\"violence_model.h5\")\n",
    "\n",
    "print(\"âœ… Training complete. Model saved as violence_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np, os\n",
    "from tensorflow.keras.models import load_model\n",
    "import requests\n",
    "\n",
    "# ===== Load model =====\n",
    "model = load_model(\"violence_model.h5\")\n",
    "\n",
    "IMG_SIZE = 112\n",
    "SEQUENCE_LEN = 20\n",
    "\n",
    "def predict_and_popup(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while len(frames) < SEQUENCE_LEN and cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_resized = cv2.resize(frame, (IMG_SIZE, IMG_SIZE)).astype(\"float32\") / 255.0\n",
    "        frames.append(frame_resized)\n",
    "    cap.release()\n",
    "\n",
    "    label = \"Video too short\"\n",
    "    if len(frames) == SEQUENCE_LEN:\n",
    "        frames = np.expand_dims(frames, axis=0)\n",
    "        pred = model.predict(frames)\n",
    "\n",
    "        # Show raw predictions for debugging\n",
    "        print(\"Raw prediction:\", pred)\n",
    "\n",
    "        # âš  Fix: swapped index mapping\n",
    "        label = \"Violence\" if np.argmax(pred) == 0 else \"NonViolence\"\n",
    "\n",
    "        # Trigger popup only for Violence\n",
    "        if label == \"Violence\":\n",
    "            requests.get(f\"http://127.0.0.1:5000/test/{os.path.basename(video_path)}\")\n",
    "\n",
    "    # ===== Show overlay =====\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        color = (0,0,255) if label==\"Violence\" else (0,255,0)\n",
    "        cv2.putText(frame, f\"Prediction: {label}\", (20, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)\n",
    "        cv2.imshow(\"Prediction Result\", frame)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return label\n",
    "\n",
    "# ===== Test Example =====\n",
    "print(\"Prediction:\", predict_and_popup(\"TestVideos/vt1.mp4\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1024c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import requests\n",
    "from collections import deque\n",
    "\n",
    "# ===== Load model =====\n",
    "model = load_model(\"violence_model.h5\")\n",
    "\n",
    "IMG_SIZE = 112\n",
    "SEQUENCE_LEN = 20\n",
    "\n",
    "# Use deque to maintain a sliding window of frames\n",
    "frames = deque(maxlen=SEQUENCE_LEN)\n",
    "\n",
    "# ===== Start webcam =====\n",
    "cap = cv2.VideoCapture(0)  # 0 = default laptop webcam\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Cannot open webcam\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Preprocess frame\n",
    "    frame_resized = cv2.resize(frame, (IMG_SIZE, IMG_SIZE)).astype(\"float32\") / 255.0\n",
    "    frames.append(frame_resized)\n",
    "\n",
    "    label = \"Waiting...\"\n",
    "    color = (255, 255, 0)\n",
    "\n",
    "    # Predict only if we have enough frames\n",
    "    if len(frames) == SEQUENCE_LEN:\n",
    "        input_frames = np.expand_dims(np.array(frames), axis=0)\n",
    "        pred = model.predict(input_frames)\n",
    "        # âš  Fix: check your model's output order\n",
    "        label = \"Violence\" if np.argmax(pred) == 0 else \"NonViolence\"\n",
    "\n",
    "        # Trigger popup only for Violence\n",
    "        if label == \"Violence\":\n",
    "            try:\n",
    "                requests.get(\"http://127.0.0.1:5000/test/live\")  # adjust your endpoint\n",
    "            except:\n",
    "                pass\n",
    "        color = (0, 0, 255) if label == \"Violence\" else (0, 255, 0)\n",
    "\n",
    "    # Overlay prediction\n",
    "    cv2.putText(frame, f\"Prediction: {label}\", (20, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)\n",
    "    cv2.imshow(\"Live Violence Detection\", frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
